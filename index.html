<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-hk">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="我啊lkjflaj">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="我啊lkjflaj">
<meta property="og:locale" content="zh-hk">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="我啊lkjflaj">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>我啊lkjflaj</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-hk">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">我啊lkjflaj</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            歸檔
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/05/SVM-集成学习-Xmind/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/05/SVM-集成学习-Xmind/" itemprop="url">SVM 集成学习 Xmind</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-09-05T17:46:01+10:00">
                2019-09-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SVM-集成学习-Xmind"><a href="#SVM-集成学习-Xmind" class="headerlink" title="SVM 集成学习 Xmind"></a>SVM 集成学习 Xmind</h1><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p><img src="/images/SVM-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Xmind/SVM.png" alt="image"></p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p><img src="/images/SVM-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Xmind/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0aggregation.png" alt="image"></p>
<h2 id="SVM-model-总结"><a href="#SVM-model-总结" class="headerlink" title="SVM model 总结"></a>SVM model 总结</h2><p><img src="/images/SVM-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Xmind/model%E6%80%BB%E7%BB%93.png" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/04/c3 决策树算法比较/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/04/c3 决策树算法比较/" itemprop="url">c3 决策树算法比较</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-09-04T15:15:37+10:00">
                2019-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>看不到图片就点<a href="http://note.youdao.com/noteshare?id=5f1ed4fbfb207e92a24da03d9716564e" target="_blank" rel="noopener">有道云笔记</a></p>
<p>[TOC]</p>
<h1 id="CART-C4-5-ID3-比较"><a href="#CART-C4-5-ID3-比较" class="headerlink" title="CART, C4.5, ID3 比较"></a>CART, C4.5, ID3 比较</h1><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><p>信息增益主导<br><img src="http://note.youdao.com/yws/res/1939/A6018F5CDE48462D80CF7B327E8B1D4D" alt="image"><br>ID3 容易导致一个问题就是加入14个样本，14个都不同种类，算法的结果会是分成14个分支，不容易泛化，由此衍生出C4.5的算法</p>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>信息增益率主导<br><img src="http://note.youdao.com/yws/res/1940/F61819683F3F48EFBD95FD83C2E5F156" alt="image">  </p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>Gini系数主导<br>Gini系数反应的是剩余数据集的纯度，如果Gini系数越小就说明数据集D纯度越高<br><img src="http://note.youdao.com/yws/res/1936/26A52AE8549F452980DDA22E007422B2" alt="image"><br><img src="http://note.youdao.com/yws/res/1927/701F84B181A444D89C42F1E159035D2B" alt="image">  与信息增益公式类似：<br><img src="http://note.youdao.com/yws/res/1951/AA6EA14A7A8F4AE59852E32267CC23F2" alt="image"><br>==加入特征X后数据不纯度的减少程度，也就是取△Gini最大的那个  ==<br><img src="http://note.youdao.com/yws/res/1955/06D1ACCFD7BB435EBBC01B91EC7BAECA" alt="image"><br>特征X的计算公式(在这里是F)</p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p>CART可以做回归和分类，C4.5只能做分类，C4.5可有多个结点，CART只有两个子树<br><img src="http://note.youdao.com/yws/res/1967/947246533BE843D9987C2A4D879E7353" alt="image"><br><img src="http://note.youdao.com/yws/res/1969/8EFC4F2590B0404DB12912C64054F247" alt="image"> </p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.cnblogs.com/wxquare/p/5379970.html" target="_blank" rel="noopener">https://www.cnblogs.com/wxquare/p/5379970.html</a> 比较<br><a href="https://zhuanlan.zhihu.com/p/78709121" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/78709121</a><br><a href="https://blog.csdn.net/jason7323/article/details/78590996" target="_blank" rel="noopener">https://blog.csdn.net/jason7323/article/details/78590996</a><br><a href="https://www.zhihu.com/question/27205203/answer/148900663" target="_blank" rel="noopener">https://www.zhihu.com/question/27205203/answer/148900663</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/04/C3 信息增益/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/04/C3 信息增益/" itemprop="url">C3 信息增益</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-09-04T15:13:13+10:00">
                2019-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>看不到图片就点<a href="http://note.youdao.com/noteshare?id=ff384d601d3e4b73339e6807c8611f05" target="_blank" rel="noopener">有道云笔记</a></p>
<p>==为了对信息(特征？)进行量化处理，然后再选定以什么作为标准，期望(纯度越来越高)信息增益越大==</p>
<p>[TOC]</p>
<h1 id="决策树-特征选择"><a href="#决策树-特征选择" class="headerlink" title="决策树 特征选择"></a>决策树 特征选择</h1><p>“信息熵”是度量样本集合不确定度（纯度）的最常用的指标<br>熵—-&gt;不确定性的量度<br>熵越高，不确定性越大<br>越随机，概率低，熵越大<br>熵越大，信息量越大</p>
<h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1. 信息熵"></a>1. 信息熵</h2><p>信息熵公式:</p>
<p><img src="http://note.youdao.com/yws/res/1543/6968E3A8CA7448C3866A30344713D39A" alt="image"><br>其中p(x<sub>i</sub>)代表随机事件X为x<sub>i</sub>的概率</p>
<p>信息量：(时间发生概率越大，信息量越小，如太阳从东边升起，信息量小)<br>对于两个独立事件x, y来说<br>信息量x, y同时发生时的信息量H(x,y) = H(x) + H(y)<br>而x, y同时发生概率等于P(x,y) = p(x) * p(y)<br>(当为对数关系时，才有log(xy) = log(x) + log(y))<br>所以有<img src="http://note.youdao.com/yws/res/1565/306D00BF4CA04205A8A0C8E6FC34F5E6" alt="image">  </p>
<p>为啥是以2为低？  传统。。。。(只要保证0-1之间能正确表达概率越小，信息量越大即可)</p>
<p>为啥是负的关系？ 保证为&gt;=0</p>
<p>信息熵指的是对可能出现的信息量的==期望==（也就是不确定性）：<br><img src="http://note.youdao.com/yws/res/1575/01943F701C8647AC8E36861D0A37E5D0" alt="image"><br>转换即得到一开始的公式。  </p>
<p><img src="http://note.youdao.com/yws/res/1580/8DFD21C350874396965354D589A39FB1" alt="image"><br>越复杂，对应于每种情况的概率就越小，信息熵就越大(不确定性越大)</p>
<h2 id="2-条件熵"><a href="#2-条件熵" class="headerlink" title="2. 条件熵"></a>2. 条件熵</h2><p>条件熵定义： X给定条件下，Y的条件概率分布的熵对X的数学期望  </p>
<p>这个条件熵，是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少，变量的不确定性是多少？<br>就是有条件的信息熵，在信息熵的基础上加上条件<br>P(y|x) = …..<br><img src="http://note.youdao.com/yws/res/1604/E2D8EBC6E15A416DB664B01C3E42B8DE" alt="image"><br>注意这里的x和y都是变量，所以有多个值<br>先把条件分解，再求每个条件下的H(x)，以及该条件的概率p(x)，然后对应乘起来再相加即可。<br>具体例子看 <a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26551798</a></p>
<h2 id="3-信息增益"><a href="#3-信息增益" class="headerlink" title="3. 信息增益"></a>3. 信息增益</h2><p><strong>在决策树算法的学习过程中，信息增益是特征选择的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，相应的信息增益也就越大。</strong></p>
<p><strong>信息增益 = 信息熵 - 条件熵</strong><br>得到的结果越大，说明信息增益越大</p>
<p>信息增益代表着在一个条件下，信息复杂度（不确定性）减少的程度。  </p>
<p>那么对应到决策树结点上，怎么选择特征，就是选了这个特征后，信息增益最大(不确定性减少程度最大)，就是分支里的每一个都是一类(err rate最小)</p>
<h1 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h1><p><a href="https://zhuanlan.zhihu.com/p/26486223" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26486223</a> ID3<br><a href="https://zhuanlan.zhihu.com/p/26760551" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26760551</a> 信息熵<br><a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26551798</a> 条件熵<br><a href="https://zhuanlan.zhihu.com/p/26596036" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26596036</a> 信息增益</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/18/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/18/GBDT/" itemprop="url">GBDT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-18T13:26:32+10:00">
                2019-07-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>加载不了图片就点<a href="http://note.youdao.com/noteshare?id=f5c5f362ed69fc8881bf7f37ca0b6a59" target="_blank" rel="noopener">有道云笔记</a></p>
<h1 id="机器学习技法-——-Gradient-Boosted-Decison-Tree"><a href="#机器学习技法-——-Gradient-Boosted-Decison-Tree" class="headerlink" title="机器学习技法 —— Gradient Boosted Decison Tree"></a>机器学习技法 —— Gradient Boosted Decison Tree</h1><p>==这一章，从第一小节已知延伸到最后的GBDT，其中每一小节都包含了一个不同的aggregation==</p>
<h3 id="1-AdaBoost-Decision-Tree"><a href="#1-AdaBoost-Decision-Tree" class="headerlink" title="1. AdaBoost Decision Tree"></a>1. AdaBoost Decision Tree</h3><p><img src="http://note.youdao.com/yws/res/1168/6990ACD6EF6044BDB73EFE3A6E24F2DF" alt="image"><br>首先，比对比对上图左右两个，RF和AdaBoost DT,区别就在于base algorithm用的是bagging还是AdaBoost。这节，我们来探讨AdaBoost DT及其延伸带来的东西。</p>
<p>那么，对于AdaBoost-DT，我们不一样的地方就是权重问题。<br><img src="http://note.youdao.com/yws/res/1178/E645466D38664FA194F692588C6B9AFC" alt="image"><br>AdaBoost会对数据进行放大缩小(re-weight操作)，然而CART中并没有和weight有关的东西，那么我们怎么在这情况下，让CART运用这些权重呢？  </p>
<p>由上图第一个公式可以看到，我们用的是对每个犯错点都乘以权重（想办法最小化根据权重加权过的这个Ein），好了，现在的问题是，怎么让接下来的算法运用这些加权过的Ein呢，一个笨方法就是把接下来的算法中所有关于Ein的都展开然后一个个带进去，显然林老师不会用这种。  </p>
<p>那么我们可以根据权重来对数据进行sampling，什么意思呢，就是按照权重，有条件的随机抽样，使得最后的抽样结果D’的数据权重比例跟boostrap得来的权重差不多，而与此同时，DT的算法作为黑盒子存在，不改变它的任何东西，最后得到上图下面的那一串，AdaBoost + sampling + DTree 。</p>
<p><img src="http://note.youdao.com/yws/res/1202/C1A5B08770884F459BDCC61AD9B76041" alt="image"><br>那么，ada-Dtree也有和DTree一样的问题，就是当数据集x都不一样的时候，切割刀最后就是每一个x都各自作为一个结点，那样子Ein肯定为0啦，带权重的Ein也自然为0，错误率为0，然后权重就会趋向于无穷(根据上面错误率公式),那么避免这个方法就是剪枝(pruning或者直接限制树的高度)，还有就是sampling，就是指选取部分的数据集作为样本，而这一操作在ADA-DTree中已经有体现了，也就是说它可以减缓overfit的情况。  </p>
<p>上面说了，我们可以通过限制树的高度来避免权重过大，==那么如果把树的高度限制为1会出现什么事情==<br><img src="http://note.youdao.com/yws/res/1214/31EC21B43D8A46CFBF6FBCD392096F70" alt="image"><br>树的高度为1，就是只切一刀，分两半，如果这个是binary-classification的话，那么这个就是AdaBoost-Stump，也就是说AdaBoost-Stump是AdaBoost-DTree的一个特例</p>
<h3 id="2-Optimization-of-AdaBoost"><a href="#2-Optimization-of-AdaBoost" class="headerlink" title="2. Optimization of AdaBoost"></a>2. Optimization of AdaBoost</h3><p><img src="http://note.youdao.com/yws/res/1222/5678DE38B5F04A708580B09B4EFDDA2A" alt="image"><br>接下来我们看看AdaBoost的进一步分析，我们知道u<sup>(t+1)</sup>是根据u<sup>(t)</sup>和方块t进行操作得来的，那么(binary-classification的话)</p>
<ul>
<li>incorrect —-&gt; y≠gt ——-&gt; y*gt == -1</li>
<li>correct —–&gt; y = gt ——&gt; y*gt == 1<br>所以我们可以吧上面的公式总结成：<br><img src="http://note.youdao.com/yws/res/1233/8AF5623FFE1B4479A2A2A946ACDCB85F" alt="image"></li>
</ul>
<p>那么u<sup>(T+1)</sup>可以看成是前T个u的乘积，也就是中间那个u<sup>(1)</sup>·∏exp(-yαg)，然后转换为连加就是exp(-y∑αg), 其中u<sup>(1)</sup> = 1/N。  </p>
<p>时间来到这里，我们recall一下AdaBoost中最后返回的G(x) = sign(∑αg)是不是跟这里的一样（图中橙色部分），这个也是代表着voting score的意思，可以看出这个是有正比的意思的。</p>
<p><img src="http://note.youdao.com/yws/res/1249/3CED7B8570EE4B47B87A52953E8191E2" alt="image"></p>
<p>对于这个voting score αg，其实我们可以看成是wϕ(x)，也就是下面写出的SVM中与margin的距离的公式(部分)，那么这个voting score可以看成是没有正规化的margin(少了||w||)。  </p>
<p> 对于这个类margin的东西，我们啊，当然是希望它又大又正啦，大—–就是离边界远，正—–就是这个分类是正确的，那么就会有exp(-yn(voting score))small，因为voting score要大又要为正，那么可以推出u<sup>(T+1)</sup>会越来越小。这个是可以达到SVM中large margin的效果  </p>
<p> ==(u<sup>(T+1)</sup>越来越小没问题，但林老师说，∑u也越来越小，我估计是因为那个exp(里面的值)在每一轮都是重新计算的，比如说这次u10，那么下一轮u11里的exp()比上一轮u10的还要小，而∑u的结果是根据最新一轮的exp()来算，所以才会导致越来越小)==</p>
<p> 然后再T+1轮过后，我们要让所有的u之和最小，这里就得出了Error Function<br> <img src="http://note.youdao.com/yws/res/1277/546F8984DB6647E0AACD579E964C80E7" alt="image"></p>
<p>这个error function 其实是err0/1的一个上边界。  </p>
<p>那么如何让<img src="http://note.youdao.com/yws/res/1278/4C29414152F6425782DAA2275D575CA0" alt="image">实现最小呢？</p>
<p>我们考虑到gradient descent<br><img src="http://note.youdao.com/yws/res/1280/B2C4608035984558AB0A4DE25CBA5791" alt="image"><br>我们仿照gradient descent公式来造一个，如上图下面的，类似的，我们找到方向gt，而这里是用h(x)表示，它是一个函数，并不是向量。  </p>
<p>接下来我们可以进行化简，就是深红色部分的拉出来成为u<sup>(T+1)</sup><sub>n</sub>, 想要得到一个最好的h，在原点进行泰勒展开后，会得到两部分，==这两部分分别对应Gradient descent的两个部分，前面Ein常数和后面下降方向*每一步长度==，最小化Ein问题就变成了最小化后面那部分的问题了。</p>
<p><img src="http://note.youdao.com/yws/res/1299/D7EAC1379A1E4D2C80C75EB3DA73DA13" alt="image"><br>找到最好的方向—-&gt;找到最好的g—-&gt;找到最好的h——&gt;最小化那个一长串的  </p>
<p>好，那么对于binary-classification来书说，h和y的结果就只有{+1,-1}两种，bane公式就可以继续变形：<br><img src="http://note.youdao.com/yws/res/1306/BD0433D51FCB4F4795B059F255E925AE" alt="image"></p>
<p>后面那一项表示的是权重 乘上 2倍的错误，这个是不是很熟悉，我们回想前几页的ppt</p>
<p><img src="http://note.youdao.com/yws/res/1310/50984520A08542E78779A8DD9C02C44C" alt="image"></p>
<p>代入此公式，就可以得到最后的结果</p>
<p><img src="http://note.youdao.com/yws/res/1314/5EED8BD6866B4967A27FCCEA1644CA5E" alt="image"></p>
<p>那么谁负责最小化这个第二项呢(第一项是常数)，答案是AdaBoost里面的base algorithm(可以返回去看看)。</p>
<p><img src="http://note.youdao.com/yws/res/1320/90FA2E62855F4F61822E1BEA18702DC6" alt="image"></p>
<p>解决了方向h后，接下来就是一次走多长的η了，这里已经找到了h所以用g(x)代替h(x)。因为这个方向h算出来可能需要很多的计算资源，那么我们能不能找到一次能走的最大步呢？——–steepest descent  </p>
<p>好，那么对于correct和incorrect的情况下，无非exp()里面为+1和-1，所以结果就有了，那么修改上面的最小化方程，再结合+1的时候是(1-ε)，-1的时候是(ε)，就有最后的公式了。  </p>
<p>那么怎么求最好的η呢，求导，=0的时候最好，算到最后会发现还是等于α。<br>总结一下，AdaBoost让base algorithm找到最好的方向，然后偷偷地也找到了steepest，前进的步数，也就是说AdaBoost的运行已经把Gradient descent所需的方向，步数给搞了出来。  </p>
<p>综上所述，AdaBoost是沿着近似函数梯度（方向是函数而不是向量），向最优方向做了最大程度的下降</p>
<p>到目前为止，这个第二节，都是在拓展AdaBoost的含义，以及跟Gradient descent的关系</p>
<h3 id="3-Gradient-Boosting"><a href="#3-Gradient-Boosting" class="headerlink" title="3. Gradient Boosting"></a>3. Gradient Boosting</h3><p>那么，AdaBoost对于exponential error(就上面那个，我没写)with binary-output的就已经证明出来了，每一步找到一个h，再把这个h写成g，然后再决定在gt上走多远(α—&gt;进行投票)<br>那么这个AdaBoost能不能用于其他类型的err上呢？</p>
<p><img src="http://note.youdao.com/yws/res/1346/1461EC1CEC6444479A3AC72F543D85EE" alt="image"></p>
<p>答案是可以的，如上图，摇身一变，用err()代替exp()便可以应用于各种err。只要符合err的曲线是平滑的就ok。</p>
<p>For example<br><img src="http://note.youdao.com/yws/res/1352/AF087026AE5946F28DDA446C3B780896" alt="image"></p>
<p>对于regression，我们所熟知的squared error。<br>这里贴出一个泰勒展开时的来源和用途不错的视频。<a href="https://www.youtube.com/watch?v=ViRvw2Hfto4" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ViRvw2Hfto4</a>   </p>
<p>我们把err()前面一项写为sn，这是个常数，代表的是当前的位置(就是Gradient中的Ein)，如果要在某个方向走一步的话，那么就需要乘上该梯度的方向分量，怎么得到呢，在零点做泰勒展开，然后去s为sn，因为这是当前位置往下走。结合squared error (s-y)^2可以得到最后的公式。  </p>
<p>那我们要做最小化，就是后面那一项最小化，就是保证他一直为负的，sn-yn &gt;0 ,那么h(x)就让他小于0。但是问题来了，最小化，那如果h(x)是无穷大怎么办，一个方向不可能说无穷大，那么我们就限制他。</p>
<p><img src="http://note.youdao.com/yws/res/1368/38D80C75A3B1459D988AA909FFB108CC" alt="image">  </p>
<p>这里有两个方法，一个笨方法就是直接对h做限制，比如说||h|| = 1，但是这样的话有条件的最优化问题不好解。  </p>
<p>那么就用第二个方法，recall拉格朗日的正则化思想，将这个h作为一个惩罚项加入公式中，如上图，那么公式可以变形，得到第二个公式，从这个公式中我们貌似可以得到点东西。</p>
<p>h-(y-s)^2为0的时候，那就最优化了，所以这个越小越好，其中(y-s)表示的是residual，距离想要的位置还有多远(y是目的地，s是目前所在)。所以我们有{(xn,yn-sn)}来做一个regression来求得h。</p>
<p><img src="http://note.youdao.com/yws/res/1380/7E99B7FA781140D19A954AB34D926AA0" alt="image"> </p>
<p>求得h后，怎么求η呢，同样的squared error嘛，err()里面两项相减再平方，稍微变形下就又有哪个(yn-sn)了，那么接下来就是一个变量的linear regression，通过η来调整gt，使得η*gt 靠近 residual</p>
<p>合起来 成就GBDT:<br><img src="http://note.youdao.com/yws/res/1387/BCD01A06C3554C8EADEC41705162C18E" alt="image"><br>第一轮，先想办法解一个regression问题，用某一个 regression algorithm来解{(xn,yn-sn)}—base learner, 比如说 DTree，因为DTree可以做regression。</p>
<p>第二轮，开始在residual上做的好还是不好</p>
<p>T轮后，会得到一堆的DTree  </p>
<p>GBDT是AdaBoost-DTree的regression版本</p>
<p>这一轮就是延伸AdaBoost</p>
<h3 id="4-summary-of-aggregation"><a href="#4-summary-of-aggregation" class="headerlink" title="4. summary of aggregation"></a>4. summary of aggregation</h3><p>总结一下：  </p>
<p><img src="http://note.youdao.com/yws/res/1420/8F142015534044E1B50DDF92B8DC4819" alt="image"></p>
<ul>
<li><p>blending: 就是先获得gt，后进行aggregate<br><img src="http://note.youdao.com/yws/res/1416/95A5533441F74E4D9E315BA1E5E318B8" alt="image"></p>
</li>
<li><p>learning: 边学习g，边aggregate<br>如下图：  </p>
</li>
</ul>
<ol>
<li>bagging—获得gt后，通过uniform形式进行投票(不需要通过其他东西辅助)  </li>
<li>AdaBoost —-通过re-weight获得gt，然后通过non-uniform形式(即linear组合)，通过steepest方式做投票，如本章第二小节内容。</li>
<li>Gradient Boost—–在AdaBoost基础上的延伸，这里以regression为例子，他是通过residual fitting得到gt，然后和AdaBoost一样，non-uniform(linear组合)以及steepest做投票。 </li>
<li>Decision Tree—–通过data splitting方式(即branch切数据)来获得gt，而后进行有条件的投票(conditional vote)通过每个分支的投票。<br>==一般来讲，linear组合的方式更受人欢迎==<br><img src="http://note.youdao.com/yws/res/1423/741882FBA4A34B74BDF6D2A4D3FCFBFB" alt="image"></li>
</ol>
<p>接下来就是总结组合情况：</p>
<ol>
<li>Bagging+DTree —&gt;  RF(这里的DTree是比较强的，用到了不同方向的投影，更加diverse，所以强)</li>
<li>AdaBoost+DTree —-&gt; AdaBoost-DTree(这里的DTree是比较弱的，因为AdaBoost的初衷就是结合弱的做的更好)</li>
<li>GradientBoost+DTree  —-&gt; GBDT(GradientBoost作为AdaBoost的延伸，自然而然的，需求也是weak的DTree)<br><img src="http://note.youdao.com/yws/res/1444/E0A2B80DE27B41D2A7F4E84ABDEBBABD" alt="image"></li>
</ol>
<p>aggregation有两个好处：</p>
<ol>
<li>解决underfit问题，因为它是由许多weak g组合而成的strong G，形成跟更复杂模型，这相当于是feature transform</li>
<li>解决overfit问题，把很多个g合起来后，会得到一个比较moderate的选择，能够阻止overfit，类似SVM的large margin表现，相当于regularization<br><img src="http://note.youdao.com/yws/res/1454/36B0C14E91034CD9805756598E0FA499" alt="image"></li>
</ol>
<p><img src="http://note.youdao.com/yws/res/1464/DBC7E7EC112145D38E1491DE44B72734" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/16/Random Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/16/Random Tree/" itemprop="url">Random Tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-16T12:40:40+10:00">
                2019-07-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>加载不了图片就点<a href="http://note.youdao.com/noteshare?id=948ef68b8ab9ec6b040a4e38ae9676b8" target="_blank" rel="noopener">有道云笔记</a></p>
<h1 id="机器学习技法-——-Random-Forest"><a href="#机器学习技法-——-Random-Forest" class="headerlink" title="机器学习技法 —— Random Forest"></a>机器学习技法 —— Random Forest</h1><p>基分类器gt</p>
<h3 id="1-Random-Forest-Alogorithm"><a href="#1-Random-Forest-Alogorithm" class="headerlink" title="1. Random  Forest Alogorithm"></a>1. Random  Forest Alogorithm</h3><p><img src="http://note.youdao.com/yws/res/953/5520C7C70C074694B4439808C07ACED2" alt="image"><br>回想Bagging, 其可以减少variance，而Decision Tree则是具有很大的variance，那么如果把他们结合起来就有Random Forest<br><img src="http://note.youdao.com/yws/res/958/031637B1F3B942DD97042BC9A44DD8ED" alt="image"><br>同时，这种方法具有相对多的好处，能够并行运算不同的决策树再合起来，RF集成了CART的优点，还有就是多个决策树共同抉择避免了overfit。<br>此时的RF = bagging + CART<br>我们知道bagging是通过对数据集有放回的采集从而做到randomness，那么现在，我们考虑在这基础上对feature动手脚。<br><img src="http://note.youdao.com/yws/res/964/DD1242E791394D26A22816FC1D1EEE03" alt="image"><br>在每一次做CART的b(x)时都重新re-sampie feature，例如每次都只从100个feature中选取10个建树，这样的话，decision tree的diversity会很大，能够更好的拟合数据。  </p>
<p><img src="http://note.youdao.com/yws/res/977/DD474018DBBC4E36BBE8104DDCD18592" alt="image"><br>说到这里，其实对于原本的特征x，我们可以把subspace看成是φ(x) = P · x，这里的P是投影矩阵的意思。<br>当这个P里面的内容保持原本各自的方向时就叫做natural basis  </p>
<table>
<thead>
<tr>
<th>d’’/feature</th>
<th>f1</th>
<th>f2</th>
<th>f3</th>
<th>f4</th>
<th>…</th>
<th>fT</th>
</tr>
</thead>
<tbody><tr>
<td>f1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>f2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>f3</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>f4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>…</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>fT</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>那么我们能不能将x投影到任意方向呢？(就是上面表格中，不再是单位矩阵，而是各种不同的数值在矩阵中，从而形成不同方向的投影矩阵)  </p>
<p>row Pi 考虑的是随机的方向，把若干个原始的feature合起来，使得一个维度上可能有不同的feature组合而成。  </p>
<table>
<thead>
<tr>
<th>d’’/feature</th>
<th>f1</th>
<th>f2</th>
<th>f3</th>
<th>f4</th>
<th>…</th>
<th>fT</th>
</tr>
</thead>
<tbody><tr>
<td>f1</td>
<td>0.2</td>
<td>0</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>f2</td>
<td>0</td>
<td>0.9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.32</td>
</tr>
<tr>
<td>f3</td>
<td>0</td>
<td>0</td>
<td>0.14</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>f4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.255</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>…</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.466</td>
<td>0</td>
</tr>
<tr>
<td>fT</td>
<td>0</td>
<td>0</td>
<td>0.24</td>
<td>0</td>
<td>0</td>
<td>0.234</td>
</tr>
</tbody></table>
<p>每一行，都只选取d’’个非0项做组合，而当d’’=1时也就是natural basis了，原作者推荐，每次做CART的b(x)时，都做一次都用P变化x。<br>那么就有RF = bagggjng + random-combination CART —— randomness everywhere</p>
<p>==来源<a href="https://my.oschina.net/hblt147/blog/1535250==" target="_blank" rel="noopener">https://my.oschina.net/hblt147/blog/1535250==</a><br><img src="http://note.youdao.com/yws/res/945/WEBRESOURCE307d208bb889754095078416257fd58f" alt="wechatimg173.png"></p>
<h3 id="2-out-of-Bag-Estimate"><a href="#2-out-of-Bag-Estimate" class="headerlink" title="2. out-of-Bag Estimate"></a>2. out-of-Bag Estimate</h3><p>out of bag(OOB)就是过N’轮选择后依然没被bootstrap选中的数据。<br><img src="http://note.youdao.com/yws/res/1037/F5B5E32D0E4247E7917D0E25A88BD833" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1038/9923491A7B104913BCB89647168176E2" alt="image"><br>当N’= N， N足够大时，无限趋近于1/e，大概1/3左右，也就是说大概有1/3的数据是完全没有选到的，所以说这是不是跟validation很像。<br><img src="http://note.youdao.com/yws/res/1042/4DE964CB63D34C1C9A765906B1315A52" alt="image"></p>
<p>上面两个图，对比，很类似，直观的看，OOB可以作为小g的验证集，然而，我们在乎的是大G，那么能不能用OOB来验证大G呢？  </p>
<p>按照上图，先对每一行操作：  (要**的 )<br>for(x1,y1) —-&gt; G<sup>-</sup><sub>1</sub> = average(g2)<br>for(x2,y2) —-&gt; G<sup>-</sup><sub>2</sub> = average(g1,g2)<br>for(x3,y3) —-&gt; G<sup>-</sup><sub>3</sub> = average(g1,g3)<br>for(xN,yN) —-&gt; G<sup>-</sup><sub>N</sub> = average(g2,g3,gT)<br>对于每一行都能得出一个子模型G<sup>-</sup><sub>N</sub>，用来验证对应的某个点，那么对于整体数据集来说，就可以用以下公式来表达：<br><img src="http://note.youdao.com/yws/res/1058/2956F993BE7349109EE2E877069BF809" alt="image"></p>
<p>好了，接下来看看validation和OOB的差别：<br><img src="http://note.youdao.com/yws/res/1064/7BF82E52F8D94D51A6F5998B6C6DA0D3" alt="image"><br>可以明显看出：</p>
<ul>
<li>validation的过程相对繁琐，先切割训练集，再用不同的hypothesis做训练然后用验证集验证，选出最好的结果以及算法再用全部数据集做训练。</li>
<li>而OOB来说只需正常进行RF即可，到最后得到了G就可以顺带得到E<sub>oob</sub>，那么这个叫做self-validation，优点是不需要重新训练</li>
</ul>
<h3 id="3-Feature-selection"><a href="#3-Feature-selection" class="headerlink" title="3. Feature selection"></a>3. Feature selection</h3><p><img src="http://note.youdao.com/yws/res/1076/7E6CF98292704736B3F351986AF1A2A1" alt="image"><br>很多时候，不是所有的feature都是必要的，比如说一些冗余的或者不相关的feature，这个时候我们就要做feature selection。<br>feature selection后可以降低计算复杂度，更容易泛化模型。但是缺点也是明显的，feature selection是一个组合问题，那么就很容易计算爆炸的情况，其次，如果做不好，容易overfit。</p>
<p>那么，该如何做feature selection？<br><img src="http://note.youdao.com/yws/res/1079/59201C61A67F4556ABA3607FBC05C486" alt="image"><br>对于 linear model来说，很简单，建模即选择，其中w就是每个feature的重要程度</p>
<p>然而对于non-linear来说就很复杂。然而对于RF来说，有较为简单的方法来做：<br><img src="http://note.youdao.com/yws/res/1089/F34A608374024ED89439A21066B4EDE4" alt="image"><br>核心思想就是random test，什么意思呢，就是如果一个feature确实是很必要的，那么往这个feature上塞点noise，结果肯定会很不同，这里我们用random value来作为噪音。  </p>
<p>那么如何选择random value？</p>
<ul>
<li>高斯分布等一类服从均匀分布的，不行，因为可能改变原本分布</li>
<li>那么我们利用bootstrap的思想，在这里叫permutation test:就是重新打乱第i个特征的数据</li>
</ul>
<p><img src="http://note.youdao.com/yws/res/1100/AED91432E67A430D801AA116BEAD7881" alt="image"><br>就上面的公式，那么问题来了，如果要计算performance(D<sup>(p)</sup>)的话就得重新训练一遍，麻烦，那么这里可以用OOB来代替表现：<br>performance(D) —–&gt; E<sub>oob</sub>(G)<br>performance(D<sup>(p)</sup>) —–&gt; E<sub>oob</sub><sup>(p)</sup>(G)<br>意思是不在训练的时候做permutate，而是在验证时做，用第i个feature下的OOB来做permutate，而后验证表现咋样。</p>
<h3 id="4-in-action"><a href="#4-in-action" class="headerlink" title="4. in action"></a>4. in action</h3><p>左边的是CART，中间的是CART+bagging，右边的是RF，t=1<br><img src="http://note.youdao.com/yws/res/1116/FD49B2AF2BB146AFA57EB780AA72E14C" alt="image"></p>
<p>t=100时，中间表示的是第100棵decision tree，注意是一颗，而右边的表示100棵decision tree共同投票组成的RF<br><img src="http://note.youdao.com/yws/res/1119/E67DA59CC6834B3D96EAB28BC9B4E3E1" alt="image"><br><img src="http://note.youdao.com/yws/res/1122/78F703E42C904EE1B1F91B81767DF861" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1124/5B428A3EF9414AF28643495DDD33C017" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1126/F231868177A445069B142416010EBC9C" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1125/D772B42531EC44E98DD3FA221AF7066A" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1128/4884B9DB37FF43B3A23F1A0FF3E7854F" alt="image"><br>可以看到RF的线越来越平滑，而且有点像SVM</p>
<p>对于一个有噪音的数据, RF也可以尽量减少噪音的影响：<br><img src="http://note.youdao.com/yws/res/1137/FD480310724A4A47B3FB8421E44B3824" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1135/20BCC3260BD8479EAB2AA2A7156476E6" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1138/9BB6466F42994ED88A0239F53C6EB6A0" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1136/54E56FAA87BD4DC38E00BA55EC6EB2A6" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1139/2B84B3CE144D4796B691A110FA563BB3" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/1134/E9C9428D450A4D86B045BFED31121D9A" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/11/Decision Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/11/Decision Tree/" itemprop="url">Decision Tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-11T16:28:21+10:00">
                2019-07-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>看不到图片的话点这个<a href="http://note.youdao.com/noteshare?id=9086b2ed571ea2a0eab04adb1186e72f" target="_blank" rel="noopener">有道云笔记</a></p>
<h1 id="机器学习技法-—-Decision-Tree"><a href="#机器学习技法-—-Decision-Tree" class="headerlink" title="机器学习技法  —- Decision Tree"></a>机器学习技法  —- Decision Tree</h1><p>回顾之前，总结一下<br><img src="http://note.youdao.com/yws/res/762/C02F99BECA33427DA46846D60636FF34" alt="image"><br>aggregation类型有两种，原话（blending是已知小g，learning是未知小g，边学边合)。 感觉就是blending先把所有的小g求出来再合起来，而learning责任是边学边合。  </p>
<h3 id="1-Decision-Tree-Hypothesis"><a href="#1-Decision-Tree-Hypothesis" class="headerlink" title="1. Decision Tree Hypothesis"></a>1. Decision Tree Hypothesis</h3><h4 id="1-1-path-view"><a href="#1-1-path-view" class="headerlink" title="1.1 path view"></a>1.1 path view</h4><p><img src="http://note.youdao.com/yws/res/770/93652B04703B44918D7D44A98B840358" alt="image"></p>
<p>决策树就是按照人类思维去决定某一件事情，如上图，先根据下班时间来判断下一结点。公式中q代表的是箭头判断，t代表的是某条路径path，g代表的是叶子节点(这个gt(x)代表的是base hypothesis，一般用constant常数代替，因为最后要返回共同形成大G)  </p>
<h4 id="1-2-recursive-view"><a href="#1-2-recursive-view" class="headerlink" title="1.2 recursive view"></a>1.2 recursive view</h4><p><img src="http://note.youdao.com/yws/res/784/6A0673A4712443B3A115BFB7AE5D3DDE" alt="image"><br>接下来介绍了recursive view，这个比较好理解，就是从root结点出发，下面都是子树，只要一步步往下分类子树即可最终得到完整的decision tree。上图公式中，c代表哪个分支，比如说root的哪个分支c={1,2,3}，b(x) = c代表选择的是哪个该父结点下，选择哪个分支。大G<sub>c</sub>表示父结点下第c个子树。</p>
<p>Decision Tree优点嘛，就是它描述了人做决定的过程，比较simple, efficient，缺点就是缺少理论保证</p>
<p><img src="http://note.youdao.com/yws/res/798/3256DCA7D1084AD3BD9441CCE54BE09E" alt="image"></p>
<h3 id="2-Decision-Tree-Algorithm"><a href="#2-Decision-Tree-Algorithm" class="headerlink" title="2. Decision Tree Algorithm"></a>2. Decision Tree Algorithm</h3><p><img src="http://note.youdao.com/yws/res/800/BF31C45762D4454AA1DB47D0132C2E03" alt="image"><br>基本的决策树算法有以上步骤：<br>①  划定分支的条件<br>②  根据划定的分支数量把==当前数据集D==分成c份<br>③  对每个D<sub>c</sub>进行训练得到模型(递归调用方法向下跑)<br>④  把每个Gs<sub>c</sub>合起来称为大G  </p>
<p>有四个需要选择的参数：  </p>
<ul>
<li>number of branches (c)</li>
<li>branching criteria (b)</li>
<li>termination criteria (停止条件)</li>
<li>base hypothesis (gt(x)结点)  </li>
</ul>
<h4 id="2-1-Classification-and-Regression-Tree-C-amp-RT-算法"><a href="#2-1-Classification-and-Regression-Tree-C-amp-RT-算法" class="headerlink" title="2.1 Classification and Regression Tree (C&amp;RT)算法"></a>2.1 Classification and Regression Tree (C&amp;RT)算法</h4><p><img src="http://note.youdao.com/yws/res/825/0A5D1129B83546CDA35CA743148CAF20" alt="image"><br>这个算法中c = 2也就是二分树，gt(x)的取值为Ein-optimal 常数，包含两种，对于classification来说选取y最多的一项，对于regression来说，则选取y的平均值。<br>现在已经解决了4个条件中的两个，接下来解决条件b<br> <img src="http://note.youdao.com/yws/res/833/8074F3BA3D2A42D3ADE9D88DB8A1F04E" alt="image"><br> 我们先通过decision stump来进行切割，然后再用purifying来判断选取那种切割。作为上图公式，我们主要观察切开后两边的子树到底纯不纯，纯 == 一边的数据相似或相同。公式的第一项|D<sub>c</sub> with h|表示D<sub>1</sub>, D<sub>2</sub>(加权，数据量越大就更重要)。<br> 那么接下来就是怎么衡量纯度(impurity Function)?  </p>
<p> <img src="http://note.youdao.com/yws/res/849/7E53FD45B3E742D59A3AEA56AE762AED" alt="image"></p>
<ul>
<li>对于regression来说：<br>类比于返回gt的方法，可以取squared error来判断纯不纯</li>
<li>对于classification：<br>如果类比于gt的话就如上图，不等于该类别的就记录<br>那么，进一步分析classification error(上图右下公式), 1-[后面那个是纯度]，表示的是每一个不同选择里，哪一个选择最多(这种方式只考虑到一种class，对于其他类别的class来说不公平)<br>==这里的k不是分支的意思(google上叫红色石头的人说这个是分支，翻了下西瓜书，是数据集中第k类样本的意思)==<br>所以有了第三种方法，Gini index（右上)，对每一个class都计算平方然后加起来，这就就能涉猎到每个class。<br>一般来说，比较popular的就是用Gini做classification，用regression error做regression。<br>好了，剩下最后一个条件<br><img src="http://note.youdao.com/yws/res/872/FB5F94F3A49441319D8C18536E7A6D93" alt="image"><br>当在这个sub-tree中y全都相等或者x全都相等就结束，就是分无可分了。<br>贴个总的流程<br><img src="http://note.youdao.com/yws/res/875/B0B1EDAF3B364A15A5C73F36E665FF21" alt="image">  </li>
</ul>
<h3 id="3-Decision-Tree-Heuristic-in-CART"><a href="#3-Decision-Tree-Heuristic-in-CART" class="headerlink" title="3. Decision Tree Heuristic in CART"></a>3. Decision Tree Heuristic in CART</h3><p>那么，如果每个y都不一样，那么一定可以不断的切下去，到最后每个节点都是一个不同的y，Ein = 0，相对的，VC-Dimension也会很大，也就是容易overfit。这个时候就需要使用regularization了，在Decision Tree中用Pruning(剪枝处理)做regularization。<br><img src="http://note.youdao.com/yws/res/884/F0BC83A05BEE457EB124E479545F59F6" alt="image"><br>观察上面的公式，all possible G，一个树有多少种可能，很多很多。是不太容易做到全部都跑一遍的。那么我们用一种方法代替这个all possible。<br>首先求出G<sup>(0)</sup>作为fully-grown tree，之后，移除该树的一个结点，比如说G<sup>(0)</sup>是10个结点，那么现在我移除了一个，剩下9个(共有10中不同的形式，10个结点每个一次)，在这10个中我们选取Ein最小的作为G<sup>(1)</sup>。然后我们在这G<sup>(1)</sup>的基础下，再移一个结点，再比较，再选出G<sup>(2)</sup>。最终这10个结点的fully-grown tree中，我们会得到10种结点数量不一样的possible ，再将之代入公式，完美。怎么选λ？ validation，完美。  </p>
<p>当遇到非数字的数据时，既需要分类的时，也可以用决策树<br><img src="http://note.youdao.com/yws/res/897/C8B8B8699FF6414EB9F763A896602669" alt="image">  </p>
<p>那么如果遇到missing feature怎么办？<br>在Decision tree中用surrogate branch来解决<br><img src="http://note.youdao.com/yws/res/901/CE49031D4ECD4C728C09A6E5FCF62FDF" alt="image"><br>找到替代品，如用身高代替体重<br><img src="http://note.youdao.com/yws/res/904/AC9D2AD78C36452F9DB37187DCB74F28" alt="image"></p>
<h3 id="4-Decision-Tree-in-Action"><a href="#4-Decision-Tree-in-Action" class="headerlink" title="4. Decision Tree in Action"></a>4. Decision Tree in Action</h3><p><img src="http://note.youdao.com/yws/res/907/B6326F0ED054445EA33C8E986DE7C54F" alt="image"><br>切第一刀，左右两边分别作为一个子树/结点<br><img src="http://note.youdao.com/yws/res/906/4F12A7D349074CDD96F6E4A57C321117" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/909/6D9F8D27271D477EB2A52ACB2E32A0CB" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/911/1327D26783024E41AB0D69BDECC8C99F" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/913/ABEA7E4DCBA345CF875ACA5E5FC9D465" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/912/5249DDA73D984B349C1CBF5BEB2E977D" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/918/5B9F3A58B8034D238CE3C0B5BDB06D62" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/915/EAF099B3E373495DB58AFD1F3CA86949" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/916/42DD326D7F7A4189A1B16D14B1ADBD03" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/917/61585D57B9E84232A36B04008C099411" alt="image"></p>
<p>与adaBoost比较：<br><img src="http://note.youdao.com/yws/res/920/AAF8853F347E41BC8FE5269880BB493F" alt="image"><br>这里注意，Decision Tree是在当前结点(数据集)中进行切割，比如说下图中，横切一刀只切了一部分而不是整个图形都被切了，而AdaBoost只能一刀切完整个平面。<br><img src="http://note.youdao.com/yws/res/924/665DF5EE819848C8BF5BFD211207BBF2" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/09/Blending and Bagging/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/09/Blending and Bagging/" itemprop="url">Blending and Bagging</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-09T18:05:52+10:00">
                2019-07-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>看不到图片的话点这个<a href="http://note.youdao.com/noteshare?id=d0e43472dd41318aaac50f4b3e637e6f" target="_blank" rel="noopener">有道云笔记</a></p>
<h1 id="机器学习技法-—-Blending-and-Bagging"><a href="#机器学习技法-—-Blending-and-Bagging" class="headerlink" title="机器学习技法 —- Blending and Bagging"></a>机器学习技法 —- Blending and Bagging</h1><h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><p>aggregation 包括两种：blending和bagging<br>先由炒股票开始：<br>假设有T的朋友g1,g2,g3….gT,预测某只股票g(X)，那么有四种方法可选  </p>
<ul>
<li><p>选择平常表现最好的—-validation<br><img src="http://note.youdao.com/yws/res/542/7220C3DB040D4595A51D14576084210B" alt="image"></p>
</li>
<li><p>等比重(uniform)<br><img src="http://note.youdao.com/yws/res/548/F596EC1177C646F8B233D5A3D57BA1EF" alt="image"></p>
</li>
<li><p>不等比重(non-uniform)<br>这里包含了前面两种情况，validation(给最值得信任的人一票，其他人0票)<br><img src="http://note.youdao.com/yws/res/551/D4A4E007A82C49CCA1C5C3A0FCCFDAEB" alt="image"></p>
</li>
<li><p>结合条件考虑<br>这个包含前面三种情况<br><img src="http://note.youdao.com/yws/res/555/217D59DCE5164628A50800D699614716" alt="image"></p>
</li>
</ul>
<p>在这里先分析下selection by validation 的好与坏</p>
<p><img src="http://note.youdao.com/yws/res/563/DAFB49E760824824BF8D3DE889D0A59F" alt="image"></p>
<p>g<sup>-</sup>代表的是训练集中得到的g</p>
<ul>
<li>首先，validation更加的popular</li>
<li>如果选择==训练集最小的g而不是验证集最小==的g可能会导致VC-dimension的高昂代价，也有overfit风险</li>
<li>selection 需要一个strong algorithm来支持选择</li>
<li>而aggregation可以通过融合多个hypothesis，可能会得到更好的结果。<br>e.g<br><img src="http://note.youdao.com/yws/res/574/2C089CFD1E454111B7C7F5A8E67A02BE" alt="image"></li>
</ul>
<h3 id="2-uniform-Blending"><a href="#2-uniform-Blending" class="headerlink" title="2.uniform Blending"></a>2.uniform Blending</h3><h4 id="2-1-uniform-Blending-voting-for-classification"><a href="#2-1-uniform-Blending-voting-for-classification" class="headerlink" title="2.1 uniform Blending(voting) for classification"></a>2.1 uniform Blending(voting) for classification</h4><p><img src="http://note.youdao.com/yws/res/580/08D31EA53292442A94170DAC6D1AA38F" alt="image"></p>
<p>有公式  </p>
<ul>
<li>试想一下，因为每个人的票数一样，如果g<sub>t</sub>也一样，那么大G和小g就没区别了，毫无意义</li>
<li>但是，如果g<sub>t</sub>都不一样，那就有多样性了，而且结果公式和uniform的公式也一样(其中g(x)=k表示的是每个人有几票)</li>
</ul>
<p>分析下右边的图，每条线都会分隔平面，然后分别投票XX还是OO，最后中和票数</p>
<h4 id="2-2-uniform-Blending-voting-for-regression"><a href="#2-2-uniform-Blending-voting-for-regression" class="headerlink" title="2.2 uniform Blending(voting) for regression"></a>2.2 uniform Blending(voting) for regression</h4><p><img src="http://note.youdao.com/yws/res/594/9E6C4F8E681942B980CD0651DF6B8C8A" alt="image"></p>
<p>对于小g的多样性来说跟上面一样，如果小g很diverse那么其综合结果很可能会更加正确。<br>even simple uniform blending can be better than any single hypothesis<br>也就是说如果aggregation要想表现的好的话，g<sub>t</sub>必须diverse</p>
<h4 id="2-3-Uniform-Blending-的理论分析（对regression）"><a href="#2-3-Uniform-Blending-的理论分析（对regression）" class="headerlink" title="2.3 Uniform Blending 的理论分析（对regression）"></a>2.3 Uniform Blending 的理论分析（对regression）</h4><p><img src="http://note.youdao.com/yws/res/603/46F7006982FF41B3951BE6D570FE8E3B" alt="image"></p>
<p>灰色的可以略去<br>式子起源于g和真实的f的误差的平方的平均.其中,avg(g) = G, avg(f<sup>2</sup>) = 常数 = f<sup>2</sup>。  </p>
<p>最后可以得到下下面黄色的式子，可以看到表示用G计算的误差平均比用g计算的误差要小(虽然不能保证，G计算出来的结果一定比最好的g要好)</p>
<p><img src="http://note.youdao.com/yws/res/618/9CDD1C9473464053B85B06D2EB8D211D" alt="image"></p>
<p>我们考虑虚拟的迭代过程。每一轮学习中，从P<sup>N</sup>中得到N-size的训练集，然后T轮的学习中，每一轮都是使用同一分布的不同资料，那么会有上面的那条公式。  </p>
<p><img src="http://note.youdao.com/yws/res/623/FEA84CE65035416895454BDBC431AC2D" alt="image"><br>那么最终的公式有：<br>expected performance of A = expected deviation to consensus(每一个g与这个共识的差异，variance方差) + performance of consensus(bias 偏差)<br>而uniform blending的作用(用G而不是g)就是消除variance来达到stable的表现。</p>
<h3 id="3-Linear-and-any-Blending"><a href="#3-Linear-and-any-Blending" class="headerlink" title="3. Linear and any Blending"></a>3. Linear and any Blending</h3><p><img src="http://note.youdao.com/yws/res/635/B40AE3E83F9A4F4F98F5C495FD118678" alt="image"><br>对于non-uniform来说，其实就是把各个α做线性组合，那么一个好的α，必须是大于等于0的<img src="http://note.youdao.com/yws/res/638/FB3D785B2CC647BDBA88BD23A28EABB8" alt="image"><br>如上图，左边为我们的non-uniform linear blending for regression，那么我们可以将之看成上图右边的公式(SVM中学到的，probabilitic SVM–two level learning),先计算gt后再通过LinReg得到α。<br>那么综合来看就有：<br><img src="http://note.youdao.com/yws/res/644/7C87B726FEBC45ACA06DF729F64E6CEE" alt="image"><br>这里的constraint就是α&gt;=0<br>那么问题来了，与LinReg相比多了个constraint，该如何消除constraint的影响呢？<br><img src="http://note.youdao.com/yws/res/651/DCACF4FE3DFC4457B51F9BF1F82B241F" alt="image"><br>对于α &lt; 0来说，无非就是结果相反，比如错误率达到99%，那么将结果一翻就是error rate 1%(针对classification)，所以constraint的影响也就被消除了  </p>
<p>那么接下来的问题是，g怎么来，原本的g都是给定的<br><img src="http://note.youdao.com/yws/res/659/0032D594FFE740B0AEDC2E78135A71A2" alt="image"></p>
<ul>
<li>如果以Ein为标准，那么容易overfit</li>
<li>linear blending里面是包含selection作为特殊的情况，就是α满足Eval最小</li>
<li>如果Ein用来做blending，那就是aggregation of best，就会更加容易overfit。<br>所以，一般用训练集得到g，再根据Eval得到α<br><img src="http://note.youdao.com/yws/res/686/A2D0BC0592C9497D85C7BDFB246EDA13" alt="image"></li>
</ul>
<p>g<sup>-</sup>代表又训练集得到  </p>
<p>从D<sub>train</sub>训练得到g<sup>-</sup><sub>1</sub>,g<sup>-</sup><sub>2</sub>…..g<sup>-</sup><sub>T</sub>，然后再根据这些g<sup>-</sup>和Dval转换为Zn。</p>
<ul>
<li>linear Blending：<br>先计算根据上述所得(Zn,yn)来算α, ==注意，这里所使用的数据集是训练集g<sup>-</sup>==，==然后再用整个数据集结合α来进行内积运算==….<img src="http://note.youdao.com/yws/res/685/EAB7D5D4422F4FC68BBB13C90BDB339A" alt="image"></li>
<li>Any Blending(==Stacking==):<br>差不多，只不过不一定要用linear</li>
</ul>
<p><img src="http://note.youdao.com/yws/res/689/161C8D2907434BAEBFA74A9191F5F4B5" alt="image"></p>
<h3 id="4-Bagging-Bootstrap-Aggregation"><a href="#4-Bagging-Bootstrap-Aggregation" class="headerlink" title="4. Bagging(Bootstrap Aggregation)"></a>4. Bagging(Bootstrap Aggregation)</h3><p><img src="http://note.youdao.com/yws/res/692/C0AC6686F8CD413498167038F2FC4138" alt="image"></p>
<p>==这些小g到底是怎么来的，我们可不可以一遍学小g，一边决定怎么把他们合起来？==</p>
<p>怎么得到不一样的小g<br>different model<br>different parameters<br>different algorithm randomness<br>different data randomness</p>
<p>那我们能不能只用一份资料，但是制作很多小g(bootstrap)<br><img src="http://note.youdao.com/yws/res/702/C8A893A86B30464B86CF4C356B23AE9B" alt="image"><br>回想之前推倒的公式，算法的表现是由方法和偏差共同作用，就是说大家的共识比直接套一个算法往往要更好。  </p>
<p><img src="http://note.youdao.com/yws/res/706/E5ED230489574C66B381B74C933FAB46" alt="image"></p>
<p>bootstrap，从D中随机取N笔资料(抓一笔后放回再抓)，不一定是N笔资料，可以是N’笔，那么这样取出来的资料叫做bootstrap sample D~t.<br>在原本的aggregation中，我们要不断的从pN分布中得到新的数据，然后再做平均的动作<br>那么现在就是不断的bootstrap，然后根据这个副本得到数据，再进行平均<br>Bootstrap aggregation —– 也叫Bagging,这是一个meta algorithm 建立于base algorithm上<br>Bootstrap —- base algorithm<br>一个实例  </p>
<p><img src="http://note.youdao.com/yws/res/724/F0576588FAB04656B62E48B87F2ED995" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/726/735B77C854B241868EA10929757B6B7F" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/725/1232AE91D4C847C795C45B3C72D8E3BE" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/754/90B972E7FD294BFF9B761C37869E4B23" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/08/AdaBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/08/AdaBoost/" itemprop="url">AdaBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-07-08T17:09:27+10:00">
                2019-07-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>加载不了图片就点 <a href="http://note.youdao.com/noteshare?id=5f2f4d67ded0d701f27925cc70447b99" target="_blank" rel="noopener">有道云笔记分享</a></p>
<p><img src="http://note.youdao.com/yws/res/754/90B972E7FD294BFF9B761C37869E4B23" alt="image"></p>
<h1 id="机器学习技法-—-adaptive-Boosting"><a href="#机器学习技法-—-adaptive-Boosting" class="headerlink" title="机器学习技法 —- adaptive Boosting"></a>机器学习技法 —- adaptive Boosting</h1><h3 id="1-Motivation-of-Boosting"><a href="#1-Motivation-of-Boosting" class="headerlink" title="1. Motivation of Boosting"></a>1. Motivation of Boosting</h3><h6 id="首先提出了识别苹果概念"><a href="#首先提出了识别苹果概念" class="headerlink" title="首先提出了识别苹果概念"></a>首先提出了识别苹果概念</h6><hr>
<p><img src="http://note.youdao.com/yws/res/335/599F5F8FA3CD491AB25E19BADEF31314" alt="image"></p>
<hr>
<p>student  —– 代表 simple hypothesis  gt<br>class    —– 代表 sophisticalted hypothesis G<br>teacher  —– 代表 a tactic learning algorithm(==老师指出error图片，然后进行re-weight==)  </p>
<p><img src="http://note.youdao.com/yws/res/364/BFBB214B616E4800BFC06E1964D96E2A" alt="image"></p>
<hr>
<h3 id="2-Diversity-by-re-weighting"><a href="#2-Diversity-by-re-weighting" class="headerlink" title="2. Diversity by re-weighting"></a>2. Diversity by re-weighting</h3><p>从bagging说起  </p>
<hr>
<p><img src="http://note.youdao.com/yws/res/366/DAF5D95249B248959DA7ED5DB3E25070" alt="image"><br>bootstrap数据集，那么我们可能会得到重复的数据，如上图右边的，那么我们可以用上图左边的u来表示其权重. u<sup>(t)</sup><sub>n</sub> 表示第T轮的数据集中，第n的数据的权重.</p>
<p><img src="http://note.youdao.com/yws/res/382/752C2210B19D4F77911B77C9FC02CC2E" alt="image"><br>我们可以得到如上图的weighted Base Algorithm, 这个式子是通过u<sub>n</sub>来进行err的调整，那么如果通过改变小u，使得g<sub>t</sub>最小呢<br><img src="http://note.youdao.com/yws/res/386/ABDCCFD9FD514CCF8E95A7B0ECC24A8A" alt="image"><br>这里我们提出一个想法，因为g<sub>t</sub>是由u<sup>(t)</sup><sub>n</sub>得到，而g<sub>t+1</sub>是由u<sup>(t+1)</sup><sub>n</sub>得到，所以，如果说g<sub>t</sub>在t+1轮中表现不好的话，那么我们可以摒弃与g<sub>t</sub>相似的模型，而选择与g<sub>t</sub>很不一样的作为g<sub>t+1</sub></p>
<hr>
<p>那么， 新问题来了，怎么判断g<sub>t</sub>表现是否很差？</p>
<p><img src="http://note.youdao.com/yws/res/395/6699B9473837425A92ABAEB1FA356346" alt="image"></p>
<p>犹如丢硬币猜正反面，如果这个g<sub>t</sub>在t+1轮中error的概率大于等于1/2，那么就是瞎猜了</p>
<p><img src="http://note.youdao.com/yws/res/398/0ECF7D1E1CD14D7D8FB7087FD475BF8C" alt="image"></p>
<p>那这个表现差的临界点就是当==正确的数量=错误的数量==，如上图，我们可以对incorrect进行scale up操作， 对correct做scale down操作<br>具体就是对每个incorrect的点乘上6211/7337, 每个correct的点乘上1126/7337</p>
<p><img src="http://note.youdao.com/yws/res/403/D13AD9029FB84446B7BCA4C54C44974B" alt="image"></p>
<hr>
<h3 id="3-AdaBoosting-Algorithm"><a href="#3-AdaBoosting-Algorithm" class="headerlink" title="3. AdaBoosting Algorithm"></a>3. AdaBoosting Algorithm</h3><p><img src="http://note.youdao.com/yws/res/427/D645A863072D4D8388AB867DF2B87683" alt="image"><br>为了方便，我们用ε<sub>t</sub>代替原来的比例，incorrect的点就乘上方块，correct的点就除上方块。当方块&gt;=1时，ε&lt;=1/2也就是说现在这个初步算法稍微有点作用  </p>
<hr>
<p>初步算法  </p>
<p><img src="http://note.youdao.com/yws/res/434/05328D8CF46542949A18DF0F79D82352" alt="image"><br>①  根据算法和u<sup>t</sup>来算出最小Ein —–&gt;gt<br>②  根据上述方块来更新u<sup>t+1</sup><br>这个初步算法中，一般来说g<sup>1</sup>的选择为平均数(一开始每个算法的票数相等)<br>然后G(x)不能用uniform(每个人票数相等)，可以用blending方法合起来，包括linear,non-linear  </p>
<hr>
<p>然而，有一种算法，可以在计算的过程中，一边计算g一遍计算α(返回大G的时候要算)</p>
<p><img src="http://note.youdao.com/yws/res/450/96CB2439875F45B98F479980384D6D09" alt="image"></p>
<p>在上面的基础上添加一个步骤③  用ln(方块)来计算α[方块是有单调性的]<br>错误率ε越小–&gt;方块越大—-&gt;α越大<br>其次，我们也可以看到AdaBoost其实就== 一个基础的弱算法(==错误率稍微小于1/2==) + optimal re-weighting factor 方块 + 一个linear aggregation 。  </p>
<p>以下是总的adaBoost算法<br><img src="http://note.youdao.com/yws/res/468/C71E04C8656943AA9E7C47ED18F10D8D" alt="image"></p>
<p>以下是理论保证</p>
<p><img src="http://note.youdao.com/yws/res/471/42CAD70BF85A464BAB454A22ACD01F7D" alt="image"></p>
<h3 id="4-AdaBoost-in-action"><a href="#4-AdaBoost-in-action" class="headerlink" title="4. AdaBoost in action"></a>4. AdaBoost in action</h3><p>怎样选择一个weak algorithm作为开端？<br>Decision stump是一个popular choice<br>就是一个对当前的数据集切一刀</p>
<p><img src="http://note.youdao.com/yws/res/495/20DD45264AC44715B40FE0C13943D598" alt="image"></p>
<p>结合AdaBoost—-叫AdaBoost-Stump</p>
<p><img src="http://note.youdao.com/yws/res/492/1E22FFEDC7A34E319B1CFB5A9B4E756D" alt="image">  </p>
<p><img src="http://note.youdao.com/yws/res/497/C4C5D8DF08174BCDAFEF71775BCDEBDA" alt="image">  </p>
<p><img src="http://note.youdao.com/yws/res/494/FC2269A19E114E7DA718816BC5B7AE48" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/498/8D557BA192304E8E8FC7EE3F4BBB2A77" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/496/17D5069CF7214DF1888D00353CEEC424" alt="image"></p>
<p><img src="https://note.youdao.com/src/C18ABEC4E386465DB1A08F82A5E04A97" alt="image"></p>
<p><img src="http://note.youdao.com/yws/res/487/A28DF9A38CBE40C189BCF54EFF415BE0" alt="image"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/23/机器学习 吴恩达 第九周 笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/23/机器学习 吴恩达 第九周 笔记/" itemprop="url">机器学习 吴恩达 第九周 笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-04-23T21:10:21+10:00">
                2019-04-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Anomaly-detection-—–异常检测"><a href="#Anomaly-detection-—–异常检测" class="headerlink" title="Anomaly detection —–异常检测"></a>Anomaly detection —–异常检测</h1><p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/0.jpg" alt></p>
<p>造飞机引擎，然后根据其中的feature构造一个关系图，如上图，很明显，右下角那个点明显是异常</p>
<p>生活中很多东西都是符合高斯分布的，这个飞机引擎也一养</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/1.jpg" alt></p>
<p>这里主要是介绍了一下高斯分布和其公式</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/2.jpg" alt><br>这里展示了高斯分布公式中的两个变量对高斯分布的影响。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/3.jpg" alt></p>
<p>这里给出了μ的表达式和σ的表达式<br><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/4.jpg" alt></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/5.jpg" alt></p>
<p>首先，我们是通过算出p(x)是否小于某一阈值来判断他是否异常。而这里P(x) 等于每个feature的高斯分布乘积</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/6.jpg" alt></p>
<p>运行步骤，挺简单的</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/7.jpg" alt><br>这里给张例子方便理解。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/8.jpg" alt></p>
<p>假设这里给定了有标签的数据，包括异常(y=1)和正常(y=0) ,那么怎么分配呢，下图</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/9.jpg" alt></p>
<p>推荐使用第一种分配方式，如上图，异常一般是远远小于tranining set的数量的。为啥要选第一种，因为cv和test的set一般是不能有相同的</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/10.jpg" alt></p>
<p>分好了之后，就是要评估这个异常了，根据我们之前所学偏斜误差(在这里只有20个异常，所以说正确率是高的吓人，这导致偏斜误差也是很大)<br>，所以这里要用precision和recall来共同评估。这里要尝试不用的 <img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/11.jpg" alt> 使得F1-score最大。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/12.jpg" alt></p>
<p>有没有发现，supervised learning 和这个anomaly detection很像，就是training set大小不一样。</p>
<p>具体看上面。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/13.jpg" alt></p>
<p>在代入feature进关系图里后，有时候会发现它并不是高斯分布，这个时候我们就要尽量让他看起来像高斯分布，具体就是上图中的用对数或者幂来转化</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/14.jpg" alt></p>
<p>我们选择的feature，有时候并不是非常契合。怎么说呢，就是这个点在x1,x2组成的坐标系中是异常，然而用一个新的feature来判断这个点，它又是正常的。所以选择feature非常重要</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/15.jpg" alt></p>
<p>我们也可以用已有的feature整合成一个新的feature</p>
<p>接下来是</p>
<h1 id="Multivariate-Gaussian-distribution—多元高斯分布"><a href="#Multivariate-Gaussian-distribution—多元高斯分布" class="headerlink" title="Multivariate Gaussian distribution—多元高斯分布"></a>Multivariate Gaussian distribution—多元高斯分布</h1><p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/16.jpg" alt></p>
<p>这个看看就好。普通的高斯分布是中间圆圈，而多元高斯分布则是斜着的。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/17.jpg" alt><br>奉上公式。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/18.jpg" alt></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/19.jpg" alt></p>
<p>着六张图展示了不同的μ和cov(那个求和符号是协方差) 对多元高斯分布的影响。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/20.jpg" alt></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/21.jpg" alt></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/22.jpg" alt><br>好了，终于到这里为啥要选择多元的，因为多元的可以自动获取feature之间的相关性(就是之前说的用已有的feature组合成一个新的来判断)，不过只适用于m&gt;n的情况下，因为如果n&lt;m就会出现cov协方差是不可倒.</p>
<h1 id="Recommender-system-—-推荐系统"><a href="#Recommender-system-—-推荐系统" class="headerlink" title="Recommender system — 推荐系统"></a>Recommender system — 推荐系统</h1><p>推荐系统是一门很复杂的学问balabala</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/23.jpg" alt></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/24.jpg" alt><br>第二张图是一些符号的意思，我们假设要预测人们对电影的评分，图一已经给出了每个人对不同电影的评分(?代表没有评分)。这里的predict其实就是linear<br>regression。</p>
<p>在这里我们假设feature为action和romance两种，图一中间的用户评分使我们的目标y，现在就是要求θ，因为 <img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/20190423205245546.png" alt> = y(咱的线性回归公式)。 <img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/25.jpg" alt><br>看起来差不多是这样，然后用它来预测这个用户对其他电影的评分。来实现给他推荐电影。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/26.jpg" alt></p>
<p>第一个公式是针对一个用户，而第二个则是针对多用户，当然这些被选中用来训练的电影必须被用户评价过r(i,j) =1</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/27.jpg" alt></p>
<p>这里是利用梯度下降进行迭代时的公式</p>
<p>注意，这里是已知feature的数值和真实的y来求θ</p>
<p>接下来另一种东西就是已知θ求feature(不是协同过滤)</p>
<h1 id="Collaborative-filtering-—-协同过滤"><a href="#Collaborative-filtering-—-协同过滤" class="headerlink" title="Collaborative filtering —- 协同过滤"></a>Collaborative filtering —- 协同过滤</h1><p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/28.jpg" alt></p>
<p>如上图，要求x</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/29.jpg" alt><br>如之前求θ，这里是求feature</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/30.jpg" alt><br>注意，到目前为止都不是协同过滤，而上图最后一个θ-&gt;x-&gt;θ-&gt;x-&gt;θ。。。。则点明了协同过滤的思想，通过猜测θ来求x，然后通过求得x来求θ，如此反复横跳。</p>
<p>接下来就是协同过滤：</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/31.jpg" alt></p>
<p>看起来复杂，其实就是两条linear regression合并到一起，就是已知θ求x和已知x求θ两条合在一起。</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/32.jpg" alt><br>以上一样，不过对象换成了合并在一起后的式子。</p>
<p>求出来该有的东西后，接下来就是如何推荐电影了 <img src="!%5B%5D(/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/33.jpg" alt></p>
<p>ok，有评价过电影的用户我们知道如何给他推荐电影，那么没评价过的呢？</p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/34.jpg" alt></p>
<p>上图，用户评价哪里全是????，那么就可以用已有的用户评分来给他取个平均值，然后进行归一化。</p>
<p>然后没有评价的用户评分设为0，然后 <img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE%E7%AC%AC%E4%B9%9D%E5%91%A8%E7%AC%94%E8%AE%B0/35.jpg" alt><br>加上平均值，来给他推荐。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/21/机器学习 吴恩达 第八周 笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我啊lkjflaj">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/21/机器学习 吴恩达 第八周 笔记/" itemprop="url">机器学习 吴恩达 第八周 笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2019-04-21T14:46:42+10:00">
                2019-04-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="K均值聚类—K-mean"><a href="#K均值聚类—K-mean" class="headerlink" title="K均值聚类—K-mean"></a>K均值聚类—K-mean</h1><p>对无监督学习对象进行分类的常用方法之一。</p>
<p>因为比较简单，所以就简单说下</p>
<p><img src="https://img-blog.csdnimg.cn/20190421134914227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421134931760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>一开始先输入K(就是cluster的个数)还有training set。</p>
<p><img src="https://img-blog.csdnimg.cn/20190421134949709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>从training set中随机选取K的点作为初始的cluster centroid。</p>
<p>然后for loop：</p>
<p>① min(每个training set的点到cluster centroid的距离)，因为是根据这个点到cluster<br>centroid的距离来判断，这个点是否属于这个cluster centroid。E.g c(5) = 4 代表 第五个样本点是属于第四个cluster<br>centroid。</p>
<p>② 等到所有点都找到了自己的cluster<br>centroid后，再找出每个cluster新的中心点，具体就是将属于同一个cluster的所有点加起来，再取平均值，得到的就是新的cluster<br>centroid的坐标。E.g c(76) = 4, c(24) = 4,c(5) = 4，就是将这三个的横纵坐标加起来然后除以3得出新点坐标。</p>
<p>在将所有新的cluster centroid算出来后，如果新点跟旧点相比，差距不大，则可以结束，否则就继续循环①②(带着新点循环)。</p>
<p><img src="https://img-blog.csdnimg.cn/20190421135832303.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>如图，K-mean的目的是让costFunction J达到最小，就是training set中每个点到其cluster centroid的距离的和最小。</p>
<p>然而，随机初始化centroid可能会带来不好的影响，看图</p>
<p><img src="https://img-blog.csdnimg.cn/20190421140009370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br>如上图，如果运气差的话，初始点的位置不好，会出现以上现象。明显的3个cluster可能最后结果只有2个cluster。</p>
<p>解决方法：多次循环K-mean，然后去J的最小值时的K</p>
<p><img src="https://img-blog.csdnimg.cn/20190421140030613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br><img src="https://img-blog.csdnimg.cn/2019042114020297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>循环算K-mean后，会有这么两种结果第一种称为Elbow method，就是斜率问题。如图选择K=3，因为斜率在这里骤变。</p>
<p>然后对于图二，很难选</p>
<p><img src="https://img-blog.csdnimg.cn/20190421140532677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>这里post上一题目。K=5时&gt;K=3</p>
<p>有时候，对于K-mean要根据需求了来选择K，如下图，不同衣服size</p>
<p><img src="https://img-blog.csdnimg.cn/2019042114070073.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<h1 id="Dimensionality-Reduction—维度约简"><a href="#Dimensionality-Reduction—维度约简" class="headerlink" title="Dimensionality Reduction—维度约简"></a>Dimensionality Reduction—维度约简</h1><p>两个motivation ： 1. Data compression 数据压缩，可以提高运算速度和减少存储空间。</p>
<p>2. Data Visualization 更直观的视图</p>
<h3 id="Data-Compression"><a href="#Data-Compression" class="headerlink" title="Data Compression"></a>Data Compression</h3><p>从2D-&gt;1D</p>
<p>在这个例子里，2D坐标系中所有点可以由一条大概的直线贯穿，点到直线上的映射  (注意，这里是点到直线的距离，并不是点的纵坐标到直线的距离)<br>如下图那一条线就是点的映射，那么整个2D数据就可以由原来的(x,y)—&gt;z用z来表达</p>
<p><img src="https://img-blog.csdnimg.cn/201904211410358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>3D-&gt;2D同理，只不过用（z1,z2)来代替原本的(x,y,z)</p>
<p><img src="https://img-blog.csdnimg.cn/20190421141551213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<h3 id="Data-Visualization"><a href="#Data-Visualization" class="headerlink" title="Data Visualization"></a>Data Visualization</h3><p><img src="https://img-blog.csdnimg.cn/20190421141242493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421141236785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/2019042114124156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br>上面三图所表达的东西可以最终由GDP和人均GDP或类似的东西来表达。从原本的好多D降到了2D</p>
<h1 id="Principal-Component-Analysis-Problem-Formulation-PCA"><a href="#Principal-Component-Analysis-Problem-Formulation-PCA" class="headerlink" title="Principal Component Analysis Problem Formulation(PCA)"></a>Principal Component Analysis Problem Formulation(PCA)</h1><p>PCA的目的就是降维，具体怎么做呢，就是找到一个向量(就是那条线)，使得所有training 点映射在这向量上面的projection<br>error最小。啥是projection error，就是下图，点到直线的距离（  是距离，不是纵坐标距离）</p>
<p><img src="https://img-blog.csdnimg.cn/20190421142143961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421142050639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br>这就是上面所说的距离。左图是linear regression，其目的是预测y和原本真实的y的差距，所以用的是纵坐标上的距离。</p>
<p>而右图则是PCA，求得是降维过程中的最佳向量。</p>
<p>那咋用呢</p>
<h3 id="第一步：处理数据，feature-scaling啊-mean"><a href="#第一步：处理数据，feature-scaling啊-mean" class="headerlink" title="第一步：处理数据，feature scaling啊 mean"></a>第一步：处理数据，feature scaling啊 mean</h3><p>normalization啊什么的保证取值范围在一个合理的范围内，不然容易因为某一个的取值范围过大或过小而影响了最终结果（参考那个房子面积和房间数）</p>
<p><img src="https://img-blog.csdnimg.cn/20190421142456235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<h3 id="第二步：计算协方差矩阵-和特征向量"><a href="#第二步：计算协方差矩阵-和特征向量" class="headerlink" title="第二步：计算协方差矩阵 和特征向量"></a>第二步：计算协方差矩阵 和特征向量</h3><p><img src="https://img-blog.csdnimg.cn/20190421142729518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421142821459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br>这个协方差(covariance matrix)符号和求和符号很像，然后这里用sigma表示协方差矩阵</p>
<p>接下来就是算特征向量</p>
<p><img src="https://img-blog.csdnimg.cn/20190421143025389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt><br>svd()唤作single value decomposition奇异值分解。这里U,S,V我们只取U来玩。</p>
<p>具体原理，咱不知道，咱也不敢问。</p>
<p><img src="https://img-blog.csdnimg.cn/20190421143249764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>看到上面的U没有，我们取前K个(这里的K就是降维后的维数)，作为K个向量μ，(啥意思呢，就是如果2D-&gt;1D,<br>就只用一条向量能表达嘛，这个μ就等于那条直线，同理 3D-&gt;2D，μ1，μ2)。。然后z就是我们降维后用来代替原本维度东东。z = Ureduce *<br>X，这个Ureduce就是取了前K个后的U。</p>
<p><img src="https://img-blog.csdnimg.cn/20190421143557761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<h1 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h1><p>有降维肯定就有还原啦。</p>
<p><img src="https://img-blog.csdnimg.cn/20190421143824104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>这是第一种方法，projection error 的平均值误差 / 那个总变量得到的结果小于某个数，这里是小于0.01，换句话说，99%的变量都被保留了</p>
<p><img src="https://img-blog.csdnimg.cn/20190421143903278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>第二种方法就简单的多， 咱取U,S,V中的S，这是一个对角线有元素，其他为0的矩阵，然后按照公式，要多少k取多少k然后除总数，搞定</p>
<p><img src="https://img-blog.csdnimg.cn/20190421144039571.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<h1 id="最后补充："><a href="#最后补充：" class="headerlink" title="最后补充："></a>最后补充：</h1><p><img src="https://img-blog.csdnimg.cn/20190421144321898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421144310477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>PCA只是用来降维，不能用来解决over’fit</p>
<p><img src="https://img-blog.csdnimg.cn/20190421144320260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p>PCA不应该是作为首先考虑的手段，应该用别的方法算过后不行才用。</p>
<p>这里只放两段代码</p>
<p><img src="https://img-blog.csdnimg.cn/20190421144558123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190421144609946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l6YXJySw==,size_16,color_FFFFFF,t_70" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">文章</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分類</span>
                
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/KevinYuKun" target="_blank" title="Github">
                      
                        <i class="fa fa-fw fa-globe"></i>Github</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/YzarrK" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-globe"></i>CSDN</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 強力驅動</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主題 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
